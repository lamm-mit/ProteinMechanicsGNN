{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import PNAConv, BatchNorm, global_add_pool\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_th = 12.0  # threshold distance between CA atoms below which edges will be built\n",
    "idx_mode = 0  # first mode = 0, last mode = 63\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "num_layers = 4\n",
    "patience = 10\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "task_name = 'all_th'+str(int(dist_th))+'_mode'+str(idx_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_raw = torch.load('data/'+task_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data for data in data_list_raw if data.y < 8]  # remove outlier\n",
    "\n",
    "N_data = len(data_list)\n",
    "print(f'Number of proteins: {N_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_val_and_test = np.random.choice(N_data, int(np.floor(0.2 * N_data)), replace=False)\n",
    "idx_all = list(range(N_data))\n",
    "idx_train = list(set(idx_all) - set(idx_val_and_test))\n",
    "idx_val = idx_val_and_test[:int(np.floor(0.5*len(idx_val_and_test)))]\n",
    "idx_test = idx_val_and_test[int(np.floor(0.5*len(idx_val_and_test))):]\n",
    "\n",
    "train_dataset = [data_list[i] for i in idx_train]\n",
    "val_dataset = [data_list[i] for i in idx_val]\n",
    "test_dataset = [data_list[i] for i in idx_test]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['axes.linewidth'] = 10\n",
    "mpl.rcParams['xtick.major.size'] = 30\n",
    "mpl.rcParams['xtick.major.width'] = 10\n",
    "mpl.rcParams['ytick.major.size'] = 30\n",
    "mpl.rcParams['ytick.major.width'] = 10\n",
    "\n",
    "def plot_freqs_hist(freqs):\n",
    "    fontsize = 150\n",
    "    plt.figure(figsize=(50,40))\n",
    "    plt.gcf().subplots_adjust(left=0.2)\n",
    "    num_bins = 100\n",
    "    n, bins, patches = plt.hist(freqs, bins=num_bins)\n",
    "    plt.xlabel('Frequency (cm$^{-1}$)', fontsize=fontsize)\n",
    "    plt.ylabel('Count', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.show()\n",
    "\n",
    "def show_freqs_info(data_list):\n",
    "    freqs = torch.zeros(len(data_list), dtype=torch.float)\n",
    "    for index, data in enumerate(data_list):\n",
    "        freqs[index] = data.y\n",
    "    (max_freqs, idx_max_freqs) = torch.max(freqs, 0)\n",
    "    print(f'Maximum frequency in dataset: {max_freqs:.4f}')\n",
    "    print(f'Index of maximum frequency in dataset: {idx_max_freqs:.4f}')\n",
    "    print(f'Mean of frequencies in dataset: {torch.mean(freqs):.4f}')\n",
    "    print(f'Standard deviation of frequencies in dataset: {torch.std(freqs):.4f}')\n",
    "    plot_freqs_hist(freqs.numpy())\n",
    "\n",
    "show_freqs_info(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute in-degree histogram over training data.\n",
    "len_deg_safe = 100  # take a sufficiently large value\n",
    "deg_safe = torch.zeros(len_deg_safe, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg_safe += torch.bincount(d, minlength=deg_safe.numel())\n",
    "\n",
    "for i in range(len_deg_safe):\n",
    "    if deg_safe[i:].sum() == 0:\n",
    "        deg = deg_safe[:i]\n",
    "        break\n",
    "print(deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.node_emb = Embedding(20, 75)\n",
    "\n",
    "        aggregators = ['mean', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.batch_norms = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = PNAConv(in_channels=75, out_channels=75,\n",
    "                           aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                           edge_dim=1, towers=5, pre_layers=1, post_layers=1,\n",
    "                           divide_input=False)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(75))\n",
    "\n",
    "        self.mlp = Sequential(Linear(75, 50), ReLU(), Linear(50, 25), ReLU(), Linear(25, 1))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_emb(x.squeeze())\n",
    "\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(batch_norm(conv(x, edge_index, edge_attr.unsqueeze(1))))\n",
    "\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "\n",
    "loss_and_val_mae = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    t = time.time()\n",
    "    loss = train(epoch)\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    loss_and_val_mae.append([loss, val_mae])\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}, Time: {(time.time() - t):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(loss_and_val_mae):\n",
    "    losses_np = np.matrix(loss_and_val_mae)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss', color=color)\n",
    "    ax1.plot(range(epochs), losses_np[:, 0], color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation Mean Absolute Error (cm-1)', color=color)\n",
    "    ax2.plot(range(epochs), losses_np[:, 1], color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(loss_and_val_mae)\n",
    "\n",
    "print(f'Final Validation MAE: {loss_and_val_mae[-1][1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'train_dataset': train_dataset,\n",
    "            'val_dataset': val_dataset,\n",
    "            'test_dataset': test_dataset,\n",
    "            'deg': deg,\n",
    "            'loss_and_val_mae': loss_and_val_mae,\n",
    "            'idx_val_and_test': idx_val_and_test,\n",
    "            }, 'data/'+task_name+'_Nlayer'+str(num_layers)+'_checkpoint'+str(epochs)+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
